{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2a7278b-76a4-41fa-9db2-f17acb2ce641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper function 'load_regional' has been updated and is now ready to use.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: #The main address where all the data is stored.\n",
    "\n",
    "import os              #A tool to understand folder paths (like a GPS).\n",
    "import glob            #A tool to find all files that match a pattern (like a searchlight).\n",
    "import json            #A special tool to read the JSON file format.\n",
    "import pandas as pd    #Our most important tool for creating data tables (like a super-smart spreadsheet).\n",
    "\n",
    "#The main address where all the data is stored.\n",
    "DATA_DIR = r\"F:\\pulse-master\\data\"\n",
    "\n",
    "# Here, we define the robot and give it its instructions.\n",
    "def load_regional(category, data_type, geo_level):\n",
    "    \"\"\"\n",
    "    This function reads all JSON files from a specific path,\n",
    "    handling the different folder structures we discovered,\n",
    "    and returns a clean pandas DataFrame.\n",
    "    \"\"\"\n",
    "    path = os.path.join(DATA_DIR, category, data_type)\n",
    "\n",
    "    # --- THIS IS THE CRUCIAL FIX ---\n",
    "    # If we are loading map/transaction or map/user data, we add the 'hover' subfolder to the path.\n",
    "    if category == 'map' and data_type in ['transaction', 'user']:\n",
    "        path = os.path.join(path, 'hover')\n",
    "\n",
    "    pattern = os.path.join(path, \"country\", \"india\", geo_level, \"*\", \"*\", \"*.json\")\n",
    "\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "    if not files:\n",
    "        print(f\"--> INFO: No files found for {category}/{data_type}/{geo_level}. Skipping.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    records = []\n",
    "    for fp in files:\n",
    "        parts   = fp.split(os.sep)\n",
    "        idx     = parts.index(geo_level)+1\n",
    "        region  = parts[idx]\n",
    "        year    = parts[idx+1]\n",
    "        quarter = os.path.splitext(parts[-1])[0]\n",
    "\n",
    "        try:\n",
    "            with open(fp, 'r') as f:\n",
    "                obj = json.load(f)\n",
    "        except json.JSONDecodeError:\n",
    "            continue # Skip corrupted files\n",
    "\n",
    "        data = obj.get(\"data\", obj)\n",
    "        if isinstance(data, dict):\n",
    "            data = [data]\n",
    "        if data is None:\n",
    "            continue\n",
    "\n",
    "        for entry in data:\n",
    "            if isinstance(entry, dict):\n",
    "                entry.update({geo_level: region, \"year\": int(year), \"quarter\": int(quarter)})\n",
    "                records.append(entry)\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    if not df.empty:\n",
    "        df.columns = [str(c).strip().lower().replace(\" \", \"_\") for c in df.columns]  #This line asks, \"Do the columns 'year' AND 'quarter' both exist in my table?\" \n",
    "        if {\"year\", \"quarter\"}.issubset(df.columns):\n",
    "            q2m = {1: \"01\", 2: \"04\", 3: \"07\", 4: \"10\"}   #This dictionary is a small tool we'll use to translate the quarter number into a month number\n",
    "            df[\"date\"] = pd.to_datetime(\n",
    "                df[\"year\"].astype(str) + \"-\" + df[\"quarter\"].map(q2m) + \"-01\"      #This is where the new \"date\" column is built and added to the table.\n",
    "            )\n",
    "    return df\n",
    "\n",
    "print(\"Helper function 'load_regional' has been updated and is now ready to use.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6f7a400-ea7b-4623-8052-cab208002bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database 'phonepe_pulse' is ready.\n",
      "All 9 tables have been successfully created and cleared of old data.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Create Database and Tables\n",
    "\n",
    "import mysql.connector\n",
    "\n",
    "# --- Your Database Credentials ---\n",
    "db_host = \"127.0.0.1\"\n",
    "db_user = \"root\"\n",
    "db_pass = \"Akshay@200\"\n",
    "db_name = \"phonepe_pulse\"\n",
    "\n",
    "try:\n",
    "    # Connect without a specific database to create it first\n",
    "    con = mysql.connector.connect(host=db_host, user=db_user, password=db_pass)\n",
    "    cursor = con.cursor()\n",
    "    cursor.execute(f\"CREATE DATABASE IF NOT EXISTS {db_name}\")\n",
    "    con.database = db_name # Switch to the phonepe_pulse database\n",
    "    print(f\"Database '{db_name}' is ready.\")\n",
    "\n",
    "    # Dictionary holding all table names and their column definitions\n",
    "    tables = {\n",
    "        \"aggregated_transaction\": \"\"\"(state VARCHAR(100), year INT, quarter INT, transaction_name VARCHAR(100), transaction_count BIGINT, transaction_amount DECIMAL(30, 2), date DATE)\"\"\",\n",
    "        \"aggregated_user\": \"\"\"(state VARCHAR(100), year INT, quarter INT, brand VARCHAR(100), brand_count BIGINT, brand_percentage DECIMAL(10, 5), date DATE)\"\"\",\n",
    "        \"aggregated_insurance\": \"\"\"(state VARCHAR(100), year INT, quarter INT, insurance_name VARCHAR(100), insurance_count BIGINT, insurance_amount DECIMAL(30, 2), date DATE)\"\"\",\n",
    "        \"map_transaction\": \"\"\"(state VARCHAR(100), year INT, quarter INT, district_name VARCHAR(100), transaction_count BIGINT, transaction_amount DECIMAL(30, 2), date DATE)\"\"\",\n",
    "        \"map_user\": \"\"\"(state VARCHAR(100), year INT, quarter INT, district_name VARCHAR(100), registered_users BIGINT, app_opens BIGINT, date DATE)\"\"\",\n",
    "        \"map_insurance\": \"\"\"(state VARCHAR(100), year INT, quarter INT, district_name VARCHAR(100), insurance_count BIGINT, insurance_amount DECIMAL(30, 2), date DATE)\"\"\",\n",
    "        \"top_transaction\": \"\"\"(state VARCHAR(100), year INT, quarter INT, entity_name VARCHAR(100), entity_type VARCHAR(20), transaction_count BIGINT, transaction_amount DECIMAL(30, 2), date DATE)\"\"\",\n",
    "        \"top_user\": \"\"\"(state VARCHAR(100), year INT, quarter INT, entity_name VARCHAR(100), entity_type VARCHAR(20), registered_users BIGINT, date DATE)\"\"\",\n",
    "        \"top_insurance\": \"\"\"(state VARCHAR(100), year INT, quarter INT, entity_name VARCHAR(100), entity_type VARCHAR(20), insurance_count BIGINT, insurance_amount DECIMAL(30, 2), date DATE)\"\"\"\n",
    "    }\n",
    "\n",
    "    # Loop through the dictionary to create and clear each table\n",
    "    for table_name, columns in tables.items():\n",
    "        cursor.execute(f\"CREATE TABLE IF NOT EXISTS {table_name} {columns};\")\n",
    "        cursor.execute(f\"TRUNCATE TABLE {table_name};\")\n",
    "\n",
    "    print(\"All 9 tables have been successfully created and cleared of old data.\")\n",
    "    con.commit()   #This executes and saves the changes to the database.\n",
    "\n",
    "except mysql.connector.Error as err:\n",
    "    print(f\"DATABASE SETUP ERROR: {err}\")\n",
    "finally:\n",
    "    if 'con' in locals() and con.is_connected():\n",
    "        cursor.close()\n",
    "        con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75d3704c-dfd0-4802-bada-d588710958c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading and processing data from local files...\n",
      "SUCCESS: Data processed into a DataFrame.\n",
      "\n",
      "Step 2: Connecting to database, clearing old data, and inserting new data...\n",
      "SUCCESS: 5034 records inserted into the database.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Script for aggregated_transaction\n",
    "\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "\n",
    "\n",
    "# --- Part A: Load and Process the Data Correctly ---\n",
    "print(\"Step 1: Loading and processing data from local files...\")\n",
    "try:\n",
    "    df = load_regional(\"aggregated\", \"transaction\", \"state\")\n",
    "    processed_rows = []\n",
    "    \n",
    "    # This loop now correctly looks inside the 'paymentInstruments' key.\n",
    "    for index, row in df.iterrows():\n",
    "        for trans_data in row.get('transactiondata', []):\n",
    "            payment = trans_data.get('paymentInstruments', [{}])[0]\n",
    "            processed_rows.append([\n",
    "                row.get('state'), \n",
    "                row.get('year'), \n",
    "                row.get('quarter'), \n",
    "                trans_data.get('name'), \n",
    "                payment.get('count'), \n",
    "                payment.get('amount'), \n",
    "                row.get('date')\n",
    "            ])\n",
    "    final_df = pd.DataFrame(processed_rows, columns=['state', 'year', 'quarter', 'transaction_name', 'transaction_count', 'transaction_amount', 'date'])\n",
    "    print(\"SUCCESS: Data processed into a DataFrame.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR during data processing: {e}\")\n",
    "\n",
    "\n",
    "# --- Part B: Clear the Table and Insert the Correct Data ---\n",
    "print(\"\\nStep 2: Connecting to database, clearing old data, and inserting new data...\")\n",
    "try:\n",
    "    con = mysql.connector.connect(host=db_host, user=db_user, password=db_pass, database=db_name)\n",
    "    cursor = con.cursor()\n",
    "    \n",
    "    # We first clear the table to ensure no old, bad data remains\n",
    "    cursor.execute(\"TRUNCATE TABLE aggregated_transaction;\")\n",
    "    \n",
    "    query = \"INSERT INTO aggregated_transaction VALUES (%s, %s, %s, %s, %s, %s, %s)\"\n",
    "    data_to_insert = [tuple(row) for row in final_df.to_numpy()]\n",
    "    cursor.executemany(query, data_to_insert)\n",
    "    con.commit()\n",
    "    print(f\"SUCCESS: {cursor.rowcount} records inserted into the database.\")\n",
    "except mysql.connector.Error as err:\n",
    "    print(f\"ERROR during database insertion: {err}\")\n",
    "finally:\n",
    "    if 'con' in locals() and con.is_connected():\n",
    "        cursor.close()\n",
    "        con.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0381ca6-fd74-4326-9069-d6be2ef55883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing: aggregated_user ---\n",
      "SUCCESS: 6732 records inserted.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Process & Insert into aggregated_user\n",
    "\n",
    "print(\"\\n--- Processing: aggregated_user ---\")\n",
    "df = load_regional(\"aggregated\", \"user\", \"state\")\n",
    "processed_rows = []\n",
    "for index, row in df.iterrows():\n",
    "    if row.get('usersbydevice'):\n",
    "        for device_data in row['usersbydevice']:\n",
    "            processed_rows.append([row.get('state'), row.get('year'), row.get('quarter'), device_data.get('brand'), device_data.get('count'), device_data.get('percentage'), row.get('date')])\n",
    "\n",
    "final_df = pd.DataFrame(processed_rows, columns=['state', 'year', 'quarter', 'brand', 'brand_count', 'brand_percentage', 'date'])\n",
    "\n",
    "# Database Insertion\n",
    "try:\n",
    "    con = mysql.connector.connect(host=db_host, user=db_user, password=db_pass, database=db_name)\n",
    "    cursor = con.cursor()\n",
    "    query = \"INSERT INTO aggregated_user VALUES (%s, %s, %s, %s, %s, %s, %s)\"\n",
    "    data_to_insert = [tuple(row) for row in final_df.to_numpy()]\n",
    "    cursor.executemany(query, data_to_insert)\n",
    "    con.commit()\n",
    "    print(f\"SUCCESS: {cursor.rowcount} records inserted.\")\n",
    "except mysql.connector.Error as err:\n",
    "    print(f\"ERROR: {err}\")\n",
    "finally:\n",
    "    if 'con' in locals() and con.is_connected():\n",
    "        cursor.close()\n",
    "        con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e183e310-d807-4772-bc1a-f1b3d6400ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing: aggregated_insurance ---\n",
      "SUCCESS: 682 records inserted with correct values.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Process & Insert into aggregated_insurance (Final Corrected Version)\n",
    "\n",
    "print(\"\\n--- Processing: aggregated_insurance ---\")\n",
    "# We load the data first using our reliable helper function\n",
    "df = load_regional(\"aggregated\", \"insurance\", \"state\")\n",
    "processed_rows = []\n",
    "\n",
    "# This loop now uses the correct key 'paymentInstruments' with a capital 'I'\n",
    "for index, row in df.iterrows():\n",
    "    for ins_data in row.get('transactiondata', []):\n",
    "        # --- THIS IS THE FIX ---\n",
    "        payment = ins_data.get('paymentInstruments', [{}])[0] \n",
    "        processed_rows.append([\n",
    "            row.get('state'), \n",
    "            row.get('year'), \n",
    "            row.get('quarter'), \n",
    "            ins_data.get('name'), \n",
    "            payment.get('count'), \n",
    "            payment.get('amount'), \n",
    "            row.get('date')\n",
    "        ])\n",
    "\n",
    "final_df = pd.DataFrame(processed_rows, columns=['state', 'year', 'quarter', 'insurance_name', 'insurance_count', 'insurance_amount', 'date'])\n",
    "\n",
    "# Database Insertion (This part is correct)\n",
    "try:\n",
    "    con = mysql.connector.connect(host=db_host, user=db_user, password=db_pass, database=db_name)\n",
    "    cursor = con.cursor()\n",
    "    \n",
    "    # We first clear the table to ensure no old, bad data remains\n",
    "    cursor.execute(\"TRUNCATE TABLE aggregated_insurance;\")\n",
    "    \n",
    "    query = \"INSERT INTO aggregated_insurance VALUES (%s, %s, %s, %s, %s, %s, %s)\"\n",
    "    data_to_insert = [tuple(row) for row in final_df.to_numpy()]\n",
    "    cursor.executemany(query, data_to_insert)\n",
    "    con.commit()\n",
    "    print(f\"SUCCESS: {cursor.rowcount} records inserted with correct values.\")\n",
    "except mysql.connector.Error as err:\n",
    "    print(f\"ERROR: {err}\")\n",
    "finally:\n",
    "    if 'con' in locals() and con.is_connected():\n",
    "        cursor.close()\n",
    "        con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f044562a-4e10-4440-90c0-bc83a35e7ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing: map_transaction ---\n",
      "SUCCESS: 20604 records inserted.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Process & Insert into map_transaction (Corrected)\n",
    "\n",
    "print(\"\\n--- Processing: map_transaction ---\")\n",
    "# We load the data first using our reliable function\n",
    "df = load_regional(\"map\", \"transaction\", \"state\")\n",
    "processed_rows = []\n",
    "\n",
    "# --- THIS IS THE FIX ---\n",
    "# The correct column name, as we discovered, is 'hoverdatalist'.\n",
    "for index, row in df.iterrows():\n",
    "    for district_data in row.get('hoverdatalist', []): \n",
    "        metric = district_data.get('metric', [{}])[0]\n",
    "        processed_rows.append([\n",
    "            row.get('state'), \n",
    "            row.get('year'), \n",
    "            row.get('quarter'), \n",
    "            district_data.get('name'), \n",
    "            metric.get('count'), \n",
    "            metric.get('amount'), \n",
    "            row.get('date')\n",
    "        ])\n",
    "\n",
    "final_df = pd.DataFrame(processed_rows, columns=['state', 'year', 'quarter', 'district_name', 'transaction_count', 'transaction_amount', 'date']).dropna()\n",
    "\n",
    "# Database Insertion (This part is correct and uses batching for large data)\n",
    "try:\n",
    "    con = mysql.connector.connect(host=db_host, user=db_user, password=db_pass, database=db_name)\n",
    "    cursor = con.cursor()\n",
    "    query = \"INSERT INTO map_transaction VALUES (%s, %s, %s, %s, %s, %s, %s)\"\n",
    "    data_to_insert = [tuple(row) for row in final_df.to_numpy()]\n",
    "    batch_size = 1000\n",
    "    total_inserted = 0\n",
    "    for i in range(0, len(data_to_insert), batch_size):\n",
    "        batch = data_to_insert[i:i+batch_size]\n",
    "        if batch:\n",
    "            cursor.executemany(query, batch)\n",
    "            con.commit()\n",
    "            total_inserted += cursor.rowcount\n",
    "    print(f\"SUCCESS: {total_inserted} records inserted.\")\n",
    "except mysql.connector.Error as err:\n",
    "    print(f\"ERROR: {err}\")\n",
    "finally:\n",
    "    if 'con' in locals() and con.is_connected():\n",
    "        cursor.close()\n",
    "        con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c7e1e86-1524-46f2-b110-303cf8296e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing: map_user ---\n",
      "SUCCESS: 20608 records inserted.\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Process & Insert into map_user\n",
    "\n",
    "print(\"\\n--- Processing: map_user ---\")\n",
    "df = load_regional(\"map\", \"user\", \"state\")\n",
    "processed_rows = []\n",
    "for index, row in df.iterrows():\n",
    "    for district_name, metrics in row.get('hoverdata', {}).items():\n",
    "        processed_rows.append([row.get('state'), row.get('year'), row.get('quarter'), district_name, metrics.get('registeredUsers'), metrics.get('appOpens'), row.get('date')])\n",
    "\n",
    "final_df = pd.DataFrame(processed_rows, columns=['state', 'year', 'quarter', 'district_name', 'registered_users', 'app_opens', 'date']).dropna()\n",
    "\n",
    "# Database Insertion\n",
    "try:\n",
    "    con = mysql.connector.connect(host=db_host, user=db_user, password=db_pass, database=db_name)\n",
    "    cursor = con.cursor()\n",
    "    query = \"INSERT INTO map_user VALUES (%s, %s, %s, %s, %s, %s, %s)\"\n",
    "    data_to_insert = [tuple(row) for row in final_df.to_numpy()]\n",
    "    batch_size = 1000\n",
    "    total_inserted = 0\n",
    "    for i in range(0, len(data_to_insert), batch_size):\n",
    "        batch = data_to_insert[i:i+batch_size]\n",
    "        if batch:\n",
    "            cursor.executemany(query, batch)\n",
    "            con.commit()\n",
    "            total_inserted += cursor.rowcount\n",
    "    print(f\"SUCCESS: {total_inserted} records inserted.\")\n",
    "except mysql.connector.Error as err:\n",
    "    print(f\"ERROR: {err}\")\n",
    "finally:\n",
    "    if 'con' in locals() and con.is_connected():\n",
    "        cursor.close()\n",
    "        con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82688d1f-480a-43b4-ac5d-ca65e619ed4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing: map_insurance ---\n",
      "SUCCESS: 1043137 records inserted.\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Process & Insert into map_insurance\n",
    "\n",
    "print(\"\\n--- Processing: map_insurance ---\")\n",
    "df = load_regional(\"map\", \"insurance\", \"state\")\n",
    "processed_rows = []\n",
    "for index, row in df.iterrows():\n",
    "    for district_list in row.get('data', {}).get('data', []):\n",
    "        if len(district_list) > 3:\n",
    "            processed_rows.append([row.get('state'), row.get('year'), row.get('quarter'), district_list[3], district_list[2], 0, row.get('date')])\n",
    "\n",
    "final_df = pd.DataFrame(processed_rows, columns=['state', 'year', 'quarter', 'district_name', 'insurance_count', 'insurance_amount', 'date']).dropna()\n",
    "\n",
    "# Database Insertion\n",
    "try:\n",
    "    con = mysql.connector.connect(host=db_host, user=db_user, password=db_pass, database=db_name)\n",
    "    cursor = con.cursor()\n",
    "    query = \"INSERT INTO map_insurance VALUES (%s, %s, %s, %s, %s, %s, %s)\"\n",
    "    data_to_insert = [tuple(row) for row in final_df.to_numpy()]\n",
    "    batch_size = 1000\n",
    "    total_inserted = 0\n",
    "    for i in range(0, len(data_to_insert), batch_size):\n",
    "        batch = data_to_insert[i:i+batch_size]\n",
    "        if batch:\n",
    "            cursor.executemany(query, batch)\n",
    "            con.commit()\n",
    "            total_inserted += cursor.rowcount\n",
    "    print(f\"SUCCESS: {total_inserted} records inserted.\")\n",
    "except mysql.connector.Error as err:\n",
    "    print(f\"ERROR: {err}\")\n",
    "finally:\n",
    "    if 'con' in locals() and con.is_connected():\n",
    "        cursor.close()\n",
    "        con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6d51ae9-e601-4c9f-b66f-a455eed75869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing: top_transaction ---\n",
      "SUCCESS: 18293 records inserted.\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Process & Insert into top_transaction (Final Corrected Version)\n",
    "\n",
    "print(\"\\n--- Processing: top_transaction ---\")\n",
    "# We load the data first using our reliable helper function\n",
    "df = load_regional(\"top\", \"transaction\", \"state\")\n",
    "processed_rows = []\n",
    "\n",
    "# This loop now uses the correct key 'entityName'\n",
    "for index, row in df.iterrows():\n",
    "    # Process the list of top districts\n",
    "    for district in row.get('districts', []):\n",
    "        metric = district.get('metric', {})\n",
    "        processed_rows.append([\n",
    "            row.get('state'), \n",
    "            row.get('year'), \n",
    "            row.get('quarter'), \n",
    "            district.get('entityName'), # Correct key\n",
    "            'district', \n",
    "            metric.get('count'), \n",
    "            metric.get('amount'), \n",
    "            row.get('date') # This is the complete line\n",
    "        ])\n",
    "    # Process the list of top pincodes\n",
    "    for pincode in row.get('pincodes', []):\n",
    "        metric = pincode.get('metric', {})\n",
    "        processed_rows.append([\n",
    "            row.get('state'), \n",
    "            row.get('year'), \n",
    "            row.get('quarter'), \n",
    "            pincode.get('entityName'), # Correct key\n",
    "            'pincode', \n",
    "            metric.get('count'), \n",
    "            metric.get('amount'), \n",
    "            row.get('date') # This is the complete line\n",
    "        ])\n",
    "\n",
    "final_df = pd.DataFrame(processed_rows, columns=['state', 'year', 'quarter', 'entity_name', 'entity_type', 'transaction_count', 'transaction_amount', 'date']).dropna()\n",
    "\n",
    "# Database Insertion (This part is correct and uses batching)\n",
    "try:\n",
    "    con = mysql.connector.connect(host=db_host, user=db_user, password=db_pass, database=db_name)\n",
    "    cursor = con.cursor()\n",
    "    query = \"INSERT INTO top_transaction VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\"\n",
    "    data_to_insert = [tuple(row) for row in final_df.to_numpy()]\n",
    "    batch_size = 1000\n",
    "    total_inserted = 0\n",
    "    for i in range(0, len(data_to_insert), batch_size):\n",
    "        batch = data_to_insert[i:i+batch_size]\n",
    "        if batch:\n",
    "            cursor.executemany(query, batch)\n",
    "            con.commit()\n",
    "            total_inserted += cursor.rowcount\n",
    "    print(f\"SUCCESS: {total_inserted} records inserted.\")\n",
    "except mysql.connector.Error as err:\n",
    "    print(f\"ERROR: {err}\")\n",
    "finally:\n",
    "    if 'con' in locals() and con.is_connected():\n",
    "        cursor.close()\n",
    "        con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b830fa9-b78c-4ff2-8f6d-033c8f3b32bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing: top_user ---\n",
      "SUCCESS: 18296 records inserted.\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Process & Insert into top_user (Corrected)\n",
    "\n",
    "print(\"\\n--- Processing: top_user ---\")\n",
    "# We load the data first using our reliable helper function\n",
    "df = load_regional(\"top\", \"user\", \"state\")\n",
    "processed_rows = []\n",
    "\n",
    "# This loop now uses the correct key 'name' for both districts and pincodes\n",
    "for index, row in df.iterrows():\n",
    "    # Process the list of top districts\n",
    "    for district in row.get('districts', []):\n",
    "        processed_rows.append([\n",
    "            row.get('state'),\n",
    "            row.get('year'),\n",
    "            row.get('quarter'),\n",
    "            district.get('name'), # <-- THE FIX\n",
    "            'district',\n",
    "            district.get('registeredUsers'),\n",
    "            row.get('date')\n",
    "        ])\n",
    "    # Process the list of top pincodes\n",
    "    for pincode in row.get('pincodes', []):\n",
    "        processed_rows.append([\n",
    "            row.get('state'),\n",
    "            row.get('year'),\n",
    "            row.get('quarter'),\n",
    "            pincode.get('name'), # <-- THE FIX\n",
    "            'pincode',\n",
    "            pincode.get('registeredUsers'),\n",
    "            row.get('date')\n",
    "        ])\n",
    "\n",
    "final_df = pd.DataFrame(processed_rows, columns=['state', 'year', 'quarter', 'entity_name', 'entity_type', 'registered_users', 'date']).dropna()\n",
    "\n",
    "# Database Insertion (This part is correct and uses batching)\n",
    "try:\n",
    "    con = mysql.connector.connect(host=db_host, user=db_user, password=db_pass, database=db_name)\n",
    "    cursor = con.cursor()\n",
    "    query = \"INSERT INTO top_user VALUES (%s, %s, %s, %s, %s, %s, %s)\"\n",
    "    data_to_insert = [tuple(row) for row in final_df.to_numpy()]\n",
    "    batch_size = 1000\n",
    "    total_inserted = 0\n",
    "    for i in range(0, len(data_to_insert), batch_size):\n",
    "        batch = data_to_insert[i:i+batch_size]\n",
    "        if batch:\n",
    "            cursor.executemany(query, batch)\n",
    "            con.commit()\n",
    "            total_inserted += cursor.rowcount\n",
    "    print(f\"SUCCESS: {total_inserted} records inserted.\")\n",
    "except mysql.connector.Error as err:\n",
    "    print(f\"ERROR: {err}\")\n",
    "finally:\n",
    "    if 'con' in locals() and con.is_connected():\n",
    "        cursor.close()\n",
    "        con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8ae7d4c-8522-49c5-9873-9f1c705a0453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing: top_insurance ---\n",
      "SUCCESS: 12273 records inserted.\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Process & Insert into top_insurance (Final Corrected Version)\n",
    "\n",
    "print(\"\\n--- Processing: top_insurance ---\")\n",
    "# We load the data first using our reliable helper function\n",
    "df = load_regional(\"top\", \"insurance\", \"state\")\n",
    "processed_rows = []\n",
    "\n",
    "# This loop now uses the correct key 'entityName'\n",
    "for index, row in df.iterrows():\n",
    "    # Process the list of top districts\n",
    "    for district in row.get('districts', []):\n",
    "        metric = district.get('metric', {})\n",
    "        processed_rows.append([\n",
    "            row.get('state'),\n",
    "            row.get('year'),\n",
    "            row.get('quarter'),\n",
    "            district.get('entityName'), # Correct key\n",
    "            'district',\n",
    "            metric.get('count'),\n",
    "            metric.get('amount'),\n",
    "            row.get('date')\n",
    "        ])\n",
    "    # Process the list of top pincodes\n",
    "    for pincode in row.get('pincodes', []):\n",
    "        metric = pincode.get('metric', {})\n",
    "        processed_rows.append([\n",
    "            row.get('state'),\n",
    "            row.get('year'),\n",
    "            row.get('quarter'),\n",
    "            pincode.get('entityName'), # Correct key\n",
    "            'pincode',\n",
    "            metric.get('count'),\n",
    "            metric.get('amount'),\n",
    "            row.get('date')\n",
    "        ])\n",
    "\n",
    "final_df = pd.DataFrame(processed_rows, columns=['state', 'year', 'quarter', 'entity_name', 'entity_type', 'insurance_count', 'insurance_amount', 'date']).dropna()\n",
    "\n",
    "# Database Insertion (This part is correct and uses batching)\n",
    "try:\n",
    "    con = mysql.connector.connect(host=db_host, user=db_user, password=db_pass, database=db_name)\n",
    "    cursor = con.cursor()\n",
    "    \n",
    "    # --- THIS IS THE COMPLETE AND CORRECT LINE ---\n",
    "    query = \"INSERT INTO top_insurance VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\"\n",
    "    \n",
    "    data_to_insert = [tuple(row) for row in final_df.to_numpy()]\n",
    "    batch_size = 1000\n",
    "    total_inserted = 0\n",
    "    for i in range(0, len(data_to_insert), batch_size):\n",
    "        batch = data_to_insert[i:i+batch_size]\n",
    "        if batch:\n",
    "            cursor.executemany(query, batch)\n",
    "            con.commit()\n",
    "            total_inserted += cursor.rowcount\n",
    "            \n",
    "    print(f\"SUCCESS: {total_inserted} records inserted.\")\n",
    "except mysql.connector.Error as err:\n",
    "    print(f\"ERROR: {err}\")\n",
    "finally:\n",
    "    if 'con' in locals() and con.is_connected():\n",
    "        cursor.close()\n",
    "        con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623e9a35-5660-42a5-b938-82bd5ffa2718",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf912d63-f480-4e4d-bfe4-89644de70d8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0c4db4-581e-47f0-985f-94586053edbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
